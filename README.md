# Causal Reward Shaping for RL Trading

A reinforcement learning project that uses confounder-adjusted rewards to train more robust trading agents.

## ğŸ¯ Hypothesis

Standard RL trading agents learn from raw profit/loss (PnL), which is **confounded** by market-wide factors like VIX and overall market direction. By removing these confounders from the reward signal, we can train agents that learn genuine "alpha" (trading skill) rather than market exposure.

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # All hyperparameters
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ fetcher.py      # Download SPY, VIX data
â”‚   â”‚   â””â”€â”€ features.py     # Technical indicators (RSI, MACD, etc.)
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â””â”€â”€ trading_env.py  # Gymnasium trading environment
â”‚   â”œâ”€â”€ reward/
â”‚   â”‚   â””â”€â”€ calibrator.py   # Reward calibration (remove confounders)
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ trainer.py      # PPO training logic
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ metrics.py      # Sharpe, drawdown, regime analysis
â”œâ”€â”€ train.py                # Main training script
â”œâ”€â”€ app.py                  # Gradio demo
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train Models

```bash
# Full training (takes ~30 min on GPU)
python train.py --timesteps 500000

# Quick test run
python train.py --test
```

### 3. Launch Demo

```bash
python app.py
```

## ğŸ“Š Methodology

### The Problem

When training RL agents for trading, the reward (PnL) is confounded:

```
PnL = Î± + Î²â‚ Ã— Market_Return + Î²â‚‚ Ã— VIX_Change + Îµ
         â†‘ Confounder effects   â†‘ True signal
```

The agent learns spurious correlations (e.g., "low VIX = profit") instead of actual trading skill.

### Our Solution

We **residualize** the reward:

```python
causal_reward = raw_pnl - Î²Ì‚â‚ Ã— market_return - Î²Ì‚â‚‚ Ã— vix_change
```

This removes confounders, leaving only the "alpha" component.

### Implementation

1. **RewardCalibrator**: Fits OLS regression to estimate Î²â‚, Î²â‚‚
2. **CausalRewardWrapper**: Gymnasium wrapper that transforms rewards
3. **Two PPO agents**: Baseline (raw rewards) vs Causal (calibrated rewards)

## ğŸ“ˆ Expected Results

| Metric | Baseline PPO | Causal PPO | Improvement |
|--------|-------------|------------|-------------|
| VIX Correlation | High | Low | âœ“ |
| Regime Robustness | Variable | Stable | âœ“ |
| Sharpe Ratio | Similar | Similar | ~ |

The key win is **robustness**, not necessarily higher returns.

## ğŸ§ª Experiments

1. **E1: Baseline Comparison** - Compare metrics on test set
2. **E2: Regime Robustness** - Performance in bull/bear/sideways markets
3. **E3: VIX Sensitivity** - Correlation between returns and VIX
4. **E4: Ablation** - Remove market vs VIX adjustment separately

## ğŸ“š References

- Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*
- Ng, A. et al. (1999). *Policy invariance under reward transformations*
- Schulman, J. et al. (2017). *Proximal Policy Optimization*

## ğŸ“ License

MIT License - For academic/research use.
